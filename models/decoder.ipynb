{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "f9817eEb3JbT"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7IHYdCFj3JbU",
    "outputId": "9919ad5b-5ed2-4047-e567-f042297f174c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "zteaPoiG3Lzi",
    "outputId": "5c717610-1205-4a30-9587-18e15df3e87d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YFABFl0X3JbV"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oSuXIWQ03JbV"
   },
   "outputs": [],
   "source": [
    "class StyleController(nn.Module):\n",
    "    def __init__(self, batch_size, input_size = 8):\n",
    "         \n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.batch_size = batch_size\n",
    "        self.k = 64\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.input_size, 128, bias = True)\n",
    "        self.initialize_weights_with_he_biases_with_zero(self.fc1)\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(128)\n",
    "        \n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 128, bias = True)\n",
    "        self.initialize_weights_with_he_biases_with_zero(self.fc2)\n",
    "        \n",
    "        self.ln2 = nn.LayerNorm(128)\n",
    "        \n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(128, 4 * self.k, bias = True)\n",
    "        self.initialize_weights_with_he_biases_with_zero(self.fc3) \n",
    "        \n",
    "        self.fc4 = nn.Linear(128, 4 * self.k, bias = True)\n",
    "        self.initialize_weights_with_he_biases_with_zero(self.fc4)\n",
    "        \n",
    "        \n",
    "    def initialize_weights_with_he_biases_with_zero(self, layer : nn.Module):\n",
    "        nn.init.kaiming_normal_(layer.weight)\n",
    "        layer.bias.data.fill_(0.0)\n",
    "    \n",
    "    \"\"\"\n",
    "    x.shape = (batch_size, style_size) => by default (batch_size, 8)\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        \n",
    "        if x is None:\n",
    "            x = torch.randn((self.batch_size, self.input_size))\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.ln1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        print('module fc1 shape:', [dim for dim in x.shape])\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.ln2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        print('module fc2 shape:', [dim for dim in x.shape])\n",
    "        \n",
    "        gamma = self.fc3(x)\n",
    "        gamma = torch.reshape(gamma, [-1, 1, 1, 4 * self.k])\n",
    "        \n",
    "        print('gamma shape:', [dim for dim in gamma.shape])\n",
    "        \n",
    "        beta = self.fc4(x)\n",
    "        beta = torch.reshape(beta, [-1, 1, 1, 4 * self.k])\n",
    "        \n",
    "        print('beta shape:', [dim for dim in beta.shape])\n",
    "        \n",
    "        return torch.cat((beta, gamma), 0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wUytY2zk3JbV",
    "outputId": "9a77a194-6acd-4213-eb86-59e5a049de82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module fc1 shape: [10, 128]\n",
      "module fc2 shape: [10, 128]\n",
      "gamma shape: [10, 1, 1, 256]\n",
      "beta shape: [10, 1, 1, 256]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1, 1, 256])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "controller = StyleController(10,8)\n",
    "res = controller.forward(None)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LY119HvD3JbW"
   },
   "outputs": [],
   "source": [
    "gamma = res[0:controller.batch_size, :, :, :]\n",
    "beta = res[controller.batch_size :, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B86w5ISK3JbW",
    "outputId": "6a2e67c9-a650-4f54-901c-20edb523dffd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma shape:  torch.Size([10, 1, 1, 256])\n",
      "Beta shape:  torch.Size([10, 1, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Gamma shape: \", gamma.shape)\n",
    "print(\"Beta shape: \", beta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "y3gIry5P3JbW"
   },
   "outputs": [],
   "source": [
    "def padding2(x: torch.Tensor, pad: int, pad_mode='reflect') -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        \n",
    "        Custom padding to apply before convolution layer.\n",
    "        :param x: input image\n",
    "        :param pad: padding size\n",
    "        :param pad_mode: padding mode\n",
    "            :options:\n",
    "                - reflect\n",
    "                - zero\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        if pad_mode == 'reflect':\n",
    "            return F.pad(input=x, pad=(pad, pad, pad, pad, 0, 0, 0, 0), mode='reflect')\n",
    "\n",
    "        if pad_mode == 'zero':\n",
    "            return F.pad(input=x, pad=(pad, pad, pad, pad, 0, 0, 0, 0), mode='constant')\n",
    "\n",
    "        raise ValueError(f\"{pad_mode} must be one of ['reflect', 'zero']!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mRyK_h4d3JbW"
   },
   "outputs": [],
   "source": [
    "#t4d = torch.tensor(np.ones((2, 2, 2, 2)))\n",
    "#p3d = (0, 0, 1, 1, 1, 1) # pad by (0, 1), (2, 1), and (3, 3)\n",
    "#out = F.pad(t4d, p3d, \"constant\", value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "qGd6hMOx3JbX"
   },
   "outputs": [],
   "source": [
    "#t4d = torch.tensor(np.ones((2, 2, 2, 2)))\n",
    "#p3d = (0, 0, 1, 1, 1, 1) # pad by (0, 1), (2, 1), and (3, 3)\n",
    "#out = F.pad(t4d, (0, 0, 1, 1, 1, 1), \"reflect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "2wYtIf_p3JbX"
   },
   "outputs": [],
   "source": [
    "def upscale2dtorch(x, factor=2):\n",
    "    assert isinstance(factor, int) and factor >= 1\n",
    "    if factor == 1: return x\n",
    "    s = x.shape\n",
    "    x = torch.reshape(x, (-1, s[1], 1, s[2], 1, s[3]))\n",
    "    x = torch.tile(x, (1, 1, factor, 1, factor, 1))\n",
    "    x = torch.reshape(x, (-1, s[1] * factor, s[2] * factor, s[3]))\n",
    "    return x\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "94BKTU1x3JbX"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, style_controller):\n",
    "         \n",
    "        super().__init__()\n",
    "        \n",
    "        self.style_controller = style_controller \n",
    "        self.k = self.style_controller.k\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels = in_channels, out_channels = 4 * self.k, kernel_size=3)\n",
    "        self.initialize_weights_and_biases(self.conv1)\n",
    "        \n",
    "        self.instance_norm_layer1 = nn.InstanceNorm2d(4 * self.k)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = 4 * self.k, out_channels = 4 * self.k, kernel_size = 3, bias = False)\n",
    "        nn.init.kaiming_normal_(self.conv2.weight)\n",
    "        \n",
    "        self.instance_norm_layer2 = nn.InstanceNorm2d(4 * self.k)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels = 4 * self.k, out_channels = 4 * self.k, kernel_size = 3)\n",
    "        self.initialize_weights_and_biases(self.conv3)\n",
    "        \n",
    "        self.instance_norm_layer3 = nn.InstanceNorm2d(4 * self.k)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_channels = 4 * self.k, out_channels = 4 * self.k, kernel_size = 3, bias = False)\n",
    "        nn.init.kaiming_normal_(self.conv4.weight)\n",
    "        \n",
    "        self.instance_norm_layer4 = nn.InstanceNorm2d(4 * self.k)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(in_channels = 4 * self.k, out_channels = 4 * self.k, kernel_size = 3)\n",
    "        self.initialize_weights_and_biases(self.conv5)\n",
    "        \n",
    "        self.instance_norm_layer5 = nn.InstanceNorm2d(4 * self.k)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(in_channels = 4 * self.k, out_channels = 4 * self.k, kernel_size = 3, bias = False)\n",
    "        nn.init.kaiming_normal_(self.conv6.weight)\n",
    "        \n",
    "        self.instance_norm_layer6 = nn.InstanceNorm2d(4 * self.k)\n",
    "        \n",
    "        self.conv7 = nn.Conv2d(in_channels = 4 * self.k, out_channels = 4 * self.k, kernel_size = 5)\n",
    "        self.initialize_weights_and_biases(self.conv7)\n",
    "        \n",
    "        self.instance_norm_layer7 = nn.InstanceNorm2d(2 * self.k)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        \n",
    "        self.conv8 = nn.Conv2d(2 * self.k, self.k, 5)\n",
    "        self.initialize_weights_and_biases(self.conv8)\n",
    "        \n",
    "        self.instance_norm_layer8 = nn.InstanceNorm2d(self.k)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        \n",
    "        self.conv9 = nn.Conv2d(self.k, 3, 7)\n",
    "        self.initialize_weights_and_biases(self.conv9, True)\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    \n",
    "    def initialize_weights_and_biases(self, layer : nn.Module, bothWeightAndBias = False):\n",
    "        if not bothWeightAndBias:\n",
    "            nn.init.kaiming_normal_(layer.weight)\n",
    "            layer.bias.data.fill_(0.0)\n",
    "        else:\n",
    "            layer.weight.data.fill_(0.0)\n",
    "            layer.bias.data.fill_(0.0)\n",
    "            \n",
    "    def forward(self, styles, x):\n",
    "        \n",
    "        print(\"X shape after padding: \", padding2(x, 1, pad_mode=\"zero\").shape)\n",
    "        x_ = self.conv1(padding2(x, 1, pad_mode=\"zero\"))\n",
    "        print(\"X shape after conv: \", x_.shape)\n",
    "        x_ = self.instance_norm_layer1(x_)\n",
    "        print(\"X shape after instance norm: \", x_.shape)\n",
    "        x_ = gamma * x_ + beta\n",
    "        print(\"2: \", x_.shape)\n",
    "        x_ = self.relu1(x_)\n",
    "        \n",
    "        print(self.instance_norm_layer2(self.conv2(padding2(x_, 1, pad_mode=\"zero\"))).shape)\n",
    "        print(x.shape)\n",
    "        x += self.instance_norm_layer2(self.conv2(padding2(x_, 1, pad_mode=\"zero\")))\n",
    "        \n",
    "        print('module res{} shape:'.format(1), [dim for dim in x.shape])\n",
    "        \n",
    "        x_ = self.conv3(padding2(x, 1, pad_mode=\"zero\"))\n",
    "        x_ = self.instance_norm_layer3(x_)\n",
    "        x_ = gamma * x_ + beta\n",
    "        x_ = self.relu2(x_)\n",
    "        \n",
    "        x += self.instance_norm_layer4(self.conv4(padding2(x_, 1, pad_mode=\"zero\")))\n",
    "        \n",
    "        print('module res{} shape:'.format(2), [dim for dim in x.shape])\n",
    "        \n",
    "        x_ = self.conv5(padding2(x, 1, pad_mode=\"zero\"))\n",
    "        x_ = self.instance_norm_layer5(x_)\n",
    "        print(\"Gmma shape: \", gamma.shape)\n",
    "        x_ = gamma * x_ + beta\n",
    "        x_ = self.relu3(x_)\n",
    "        \n",
    "        x += self.instance_norm_layer6(self.conv6(padding2(x_, 1, pad_mode=\"zero\")))\n",
    "        \n",
    "        print('module res{} shape:'.format(3), [dim for dim in x.shape])\n",
    "        \n",
    "        x = upscale2d(x, 2, pad_mode=\"zero\")\n",
    "        x = self.conv7(padding2(x, 2))\n",
    "        x = self.instance_norm_layer7(x)\n",
    "        x = self.relu4(x)\n",
    "        \n",
    "        print('module deconv1 shape:', [dim for dim in x.shape])\n",
    "\n",
    "        x = upscale2d(x, 2)\n",
    "        x = self.conv8(padding2(x, 2, pad_mode=\"zero\"))\n",
    "        x = self.instance_norm_layer8(x)\n",
    "        x = self.relu5(x)\n",
    "        \n",
    "        x = self.conv9(padding2(x, 3, pad_mode=\"zero\"))\n",
    "        return self.tanh(x)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "IeIR4nMw3JbY"
   },
   "outputs": [],
   "source": [
    "controller = StyleController(10,8)\n",
    "styles = torch.tensor(np.ones((10, 8), dtype=float)).float()\n",
    "decoder = Decoder(256, controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fZ4LjAQK3JbY",
    "outputId": "e2ef481b-b875-45f2-b642-0d5d972c2881",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module fc1 shape: [10, 128]\n",
      "module fc2 shape: [10, 128]\n",
      "gamma shape: [10, 1, 1, 256]\n",
      "beta shape: [10, 1, 1, 256]\n",
      "X shape after padding:  torch.Size([10, 256, 258, 258])\n",
      "X shape after conv:  torch.Size([10, 256, 256, 256])\n",
      "X shape after instance norm:  torch.Size([10, 256, 256, 256])\n",
      "2:  torch.Size([10, 256, 256, 256])\n",
      "torch.Size([10, 256, 256, 256])\n",
      "torch.Size([10, 256, 256, 256])\n",
      "module res1 shape: [10, 256, 256, 256]\n",
      "module res2 shape: [10, 256, 256, 256]\n",
      "Gmma shape:  torch.Size([10, 1, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "decoder.forward(styles, torch.tensor(np.ones((10, 256, 256, 256))).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nyb2Jxz03JbY"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.contrib.layers import instance_norm, layer_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ulfy_g2b3JbY"
   },
   "outputs": [],
   "source": [
    "def padding(x, pad, pad_type='reflect'):\n",
    "    if pad_type == 'zero' :\n",
    "        print(\"Tf:\" , x.shape)\n",
    "        return tf.pad(x, [[0, 0], [pad, pad], [pad, pad], [0, 0]])\n",
    "    if pad_type == 'reflect' :\n",
    "        print(\"Tf: \", x.shape)\n",
    "        return tf.pad(x, [[0, 0], [pad, pad], [pad, pad], [0, 0]], mode='REFLECT')\n",
    "    else:\n",
    "        raise ValueError('Unknown pad type: {}'.format(pad_type))\n",
    "\n",
    "def conv(x, *args, pad=1, **kwargs):\n",
    "    with slim.arg_scope([slim.conv2d, slim.conv2d_transpose], padding='VALID'):\n",
    "        x = padding(x, pad)\n",
    "        print(\"X shape after padding in tf: \", x.shape)\n",
    "        return slim.conv2d(x, *args, **kwargs)\n",
    "\n",
    "def upscale2d(x, factor=2):\n",
    "    assert isinstance(factor, int) and factor >= 1\n",
    "    if factor == 1: return x\n",
    "    with tf.variable_scope('Upscale2D'):\n",
    "        s = x.shape\n",
    "        x = tf.reshape(x, [-1, s[1], 1, s[2], 1, s[3]])\n",
    "        x = tf.tile(x, [1, 1, factor, 1, factor, 1])\n",
    "        x = tf.reshape(x, [-1, s[1] * factor, s[2] * factor, s[3]])\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3l-ebWWMG2O"
   },
   "outputs": [],
   "source": [
    "class WarpController(nn.Module):\n",
    "    def __init__(self, batch_size, input_size_when_flatten, num_ldmark, scales):\n",
    "         \n",
    "        super().__init__()\n",
    "        \n",
    "        self.scales = scales\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size_when_flatten, 128, bias = True)\n",
    "        self.initialize_weights_with_he_biases_with_zero(self.fc1)\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(128)\n",
    "        \n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, num_ldmark * 2, bias = False)\n",
    "        self.init.trunc_normal_(self.fc2.weights)\n",
    "        self.fc2.bias.fill_(0.0)\n",
    "        \n",
    "        self.fc3 = nn.Linear(num_ldmark * 2, num_ldmark * 2, bias = False)\n",
    "        self.init.trunc_normal_(self.fc3.weights)\n",
    "        self.fc3.bias.fill_(0.0)\n",
    "      \n",
    "        \n",
    "    def initialize_weights_with_he_biases_with_zero(self, layer : nn.Module):\n",
    "        nn.init.kaiming_normal_(layer.weight)\n",
    "        layer.bias.data.fill_(0.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.ln1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        # BÖYLE BİR CONSTANT OLUŞTURMAK BURAYI BOZAR MI\n",
    "        ldmark_mean = (np.random.normal(0,50, (num_ldmark,2)) + np.array([[0.5*h,0.5*w]])).flatten()\n",
    "        ldmark_mean = torch.tensor(ldmark_mean, dtype=torch.float32)\n",
    "\n",
    "        ldmark_pred = self.fc2(x)\n",
    "        ldmark_pred = ldmark_pred + ldmark_mean\n",
    "        #tf.identity ile aynı mı?\n",
    "        ldmark_pred = nn.Identity(ldmark_pred)\n",
    "        ldmark_diff = self.fc3(x)\n",
    "        ldmark_diff = nn.Identity(ldmark_diff)\n",
    "        # scales i yukarda bu şekilde dahil etmek problem mi\n",
    "        ldmark_diff = nn.Identity(torch.reshape(scales, (-1, 1)) * ldmark_diff)\n",
    "        \n",
    "        src_pts = torch.reshape(ldmark_pred, (-1, num_ldmark, 2))\n",
    "        dst_pts = torch.reshape(ldmark_pred + ldmark_diff, (-1, num_ldmark, 2))\n",
    "        \n",
    "        diff_norm = torch.mean(torch.norm(src_pts - dst_pts, dim = (1, 2)))\n",
    "        images_transformed, dense_flow = sparse_image_warp(warp_input, src_pts, dst_pts, regularization_weight = 1e-6, num_boundary_points=0)\n",
    "        dense_flow = nn.Identity(dense_flow)\n",
    "        \n",
    "        # böyle multiple şey döndürmek okay mi\n",
    "        return images_transformed, images_rendered, ldmark_pred, ldmark_diff\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N13sVDW3MBT2"
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('WarpController'):\n",
    "\n",
    "                        print('-- WarpController')\n",
    "\n",
    "                        net = encoded\n",
    "                        warp_input = tf.identity(images_rendered, name='warp_input')\n",
    "\n",
    "                        net = slim.flatten(net)\n",
    "\n",
    "                        net = slim.fully_connected(net, 128, scope='fc1')\n",
    "                        print('module fc1 shape:', [dim.value for dim in net.shape])\n",
    "\n",
    "                        num_ldmark = 16\n",
    "\n",
    "                        # Predict the control points\n",
    "                        ldmark_mean = (np.random.normal(0,50, (num_ldmark,2)) + np.array([[0.5*h,0.5*w]])).flatten()\n",
    "                        ldmark_mean = tf.Variable(ldmark_mean.astype(np.float32), name='ldmark_mean')\n",
    "                        print('ldmark_mean shape:', [dim.value for dim in ldmark_mean.shape])\n",
    "\n",
    "                        ldmark_pred = slim.fully_connected(net, num_ldmark*2, \n",
    "                            weights_initializer=tf.truncated_normal_initializer(stddev=1.0),\n",
    "                            normalizer_fn=None, activation_fn=None, biases_initializer=None, scope='fc_ldmark')\n",
    "                        ldmark_pred = ldmark_pred + ldmark_mean\n",
    "                        print('ldmark_pred shape:', [dim.value for dim in ldmark_pred.shape])\n",
    "                        ldmark_pred = tf.identity(ldmark_pred, name='ldmark_pred')\n",
    "                 \n",
    "\n",
    "                        # Predict the displacements\n",
    "                        ldmark_diff = slim.fully_connected(net, num_ldmark*2, \n",
    "                            normalizer_fn=None,  activation_fn=None, scope='fc_diff')\n",
    "                        print('ldmark_diff shape:', [dim.value for dim in ldmark_diff.shape])\n",
    "                        ldmark_diff = tf.identity(ldmark_diff, name='ldmark_diff')\n",
    "                        ldmark_diff = tf.identity(tf.reshape(scales,[-1,1]) * ldmark_diff, name='ldmark_diff_scaled')\n",
    "\n",
    "\n",
    "\n",
    "                        src_pts = tf.reshape(ldmark_pred, [-1, num_ldmark ,2])\n",
    "                        dst_pts = tf.reshape(ldmark_pred + ldmark_diff, [-1, num_ldmark, 2])\n",
    "\n",
    "                        diff_norm = tf.reduce_mean(tf.norm(src_pts-dst_pts, axis=[1,2]))\n",
    "                        # tf.summary.scalar('diff_norm', diff_norm)\n",
    "                        # tf.summary.scalar('mark', ldmark_pred[0,0])\n",
    "\n",
    "                        images_transformed, dense_flow = sparse_image_warp(warp_input, src_pts, dst_pts,\n",
    "                                regularization_weight = 1e-6, num_boundary_points=0)\n",
    "                        dense_flow = tf.identity(dense_flow, name='dense_flow')\n",
    "\n",
    "                return images_transformed, images_rendered, ldmark_pred, ldmark_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTSw_nP03JbY"
   },
   "outputs": [],
   "source": [
    "def decoder(encoded, scales, styles, texture_only=False, style_size=8, image_size=(112,112),\n",
    "        keep_prob=1.0, phase_train=True, weight_decay=0.0, reuse=None, scope='Decoder'):\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        with slim.arg_scope([slim.conv2d, slim.conv2d_transpose, slim.fully_connected],\n",
    "                        activation_fn=tf.nn.relu,\n",
    "                        # weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                        weights_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                        weights_regularizer=slim.l2_regularizer(weight_decay)):\n",
    "            with slim.arg_scope([slim.dropout, slim.batch_norm], is_training=phase_train):\n",
    "                with slim.arg_scope([slim.fully_connected],\n",
    "                    normalizer_fn=layer_norm, normalizer_params=None):\n",
    "                    print('{} input shape:'.format(scope), [dim.value for dim in encoded.shape])\n",
    "                        \n",
    "                    batch_size = tf.shape(encoded)[0]\n",
    "                    h, w = tuple(image_size)\n",
    "                    k = 64\n",
    "    \n",
    "                    with tf.variable_scope('StyleController'):\n",
    "\n",
    "                        if styles is None:\n",
    "                            styles = tf.random_normal((batch_size, style_size))\n",
    "\n",
    "                        net = tf.identity(styles, name='input_style')\n",
    "\n",
    "                        net = slim.fully_connected(net, 128, scope='fc2')\n",
    "                        print('module fc2 shape:', [dim.value for dim in net.shape])\n",
    "\n",
    "                        net = slim.fully_connected(net, 128, scope='fc3')\n",
    "                        print('module fc3 shape:', [dim.value for dim in net.shape])\n",
    "\n",
    "                        gamma = slim.fully_connected(net, 4*k, activation_fn=None, normalizer_fn=None, scope='fc4')\n",
    "                        gamma = tf.reshape(gamma, [-1, 1, 1, 4*k], name='gamma')\n",
    "                        print('gamma shape:', [dim.value for dim in gamma.shape])\n",
    "\n",
    "                        beta = slim.fully_connected(net, 4*k, activation_fn=None, normalizer_fn=None, scope='fc5')\n",
    "                        beta = tf.reshape(beta, [-1, 1, 1, 4*k], name='beta')\n",
    "                        print('beta shape:', [dim.value for dim in beta.shape])\n",
    "\n",
    "\n",
    "                    \n",
    "                    with tf.variable_scope('Decoder'):\n",
    "                        print('-- Decoder')\n",
    "                        net = encoded\n",
    "\n",
    "                        adain = lambda x : gamma * instance_norm(x, center=False, scale=False) + beta\n",
    "\n",
    "                        with slim.arg_scope([slim.conv2d_transpose, slim.conv2d],\n",
    "                                    normalizer_fn=adain, normalizer_params=None):\n",
    "                            for i in range(3):\n",
    "                                print(\"net_ shape \", net.shape)\n",
    "                                net_ = conv(net, 4*k, 3, scope='res{}_0'.format(i))\n",
    "                                print('_net module res{} shape:'.format(i), [dim.value for dim in net_.shape])\n",
    "                                net += conv(net_, 4*k, 3, activation_fn=None, biases_initializer=None, scope='res{}_1'.format(i))\n",
    "                                print('net module res{} shape:'.format(i), [dim.value for dim in net.shape])\n",
    "\n",
    "               \n",
    "                        with slim.arg_scope([slim.conv2d, slim.conv2d_transpose, slim.fully_connected],\n",
    "                                normalizer_fn=layer_norm, normalizer_params=None):\n",
    "                            net = upscale2d(net, 2)\n",
    "                            net = conv(net, 2*k, 5, pad=2, scope='deconv1_1')\n",
    "                            print('module deconv1 shape:', [dim.value for dim in net.shape])\n",
    "\n",
    "                            net = upscale2d(net, 2)\n",
    "                            net = conv(net, k, 5, pad=2, scope='deconv2_1')\n",
    "\n",
    "                        net = conv(net, 3, 7, pad=3, activation_fn=None, normalizer_fn=None, \n",
    "                                    weights_initializer=tf.constant_initializer(0.0), scope='conv_image')\n",
    "                        images_rendered = tf.nn.tanh(net, name='images_rendered')\n",
    "                        print('images_rendered shape:', [dim.value for dim in images_rendered.shape])\n",
    "\n",
    "                    if texture_only:\n",
    "                        return images_rendered                        \n",
    "\n",
    "                    with tf.variable_scope('WarpController'):\n",
    "\n",
    "                        print('-- WarpController')\n",
    "\n",
    "                        net = encoded\n",
    "                        warp_input = tf.identity(images_rendered, name='warp_input')\n",
    "\n",
    "                        net = slim.flatten(net)\n",
    "\n",
    "                        net = slim.fully_connected(net, 128, scope='fc1')\n",
    "                        print('module fc1 shape:', [dim.value for dim in net.shape])\n",
    "\n",
    "                        num_ldmark = 16\n",
    "\n",
    "                        # Predict the control points\n",
    "                        ldmark_mean = (np.random.normal(0,50, (num_ldmark,2)) + np.array([[0.5*h,0.5*w]])).flatten()\n",
    "                        ldmark_mean = tf.Variable(ldmark_mean.astype(np.float32), name='ldmark_mean')\n",
    "                        print('ldmark_mean shape:', [dim.value for dim in ldmark_mean.shape])\n",
    "\n",
    "                        ldmark_pred = slim.fully_connected(net, num_ldmark*2, \n",
    "                            weights_initializer=tf.truncated_normal_initializer(stddev=1.0),\n",
    "                            normalizer_fn=None, activation_fn=None, biases_initializer=None, scope='fc_ldmark')\n",
    "                        ldmark_pred = ldmark_pred + ldmark_mean\n",
    "                        print('ldmark_pred shape:', [dim.value for dim in ldmark_pred.shape])\n",
    "                        ldmark_pred = tf.identity(ldmark_pred, name='ldmark_pred')\n",
    "                 \n",
    "\n",
    "                        # Predict the displacements\n",
    "                        ldmark_diff = slim.fully_connected(net, num_ldmark*2, \n",
    "                            normalizer_fn=None,  activation_fn=None, scope='fc_diff')\n",
    "                        print('ldmark_diff shape:', [dim.value for dim in ldmark_diff.shape])\n",
    "                        ldmark_diff = tf.identity(ldmark_diff, name='ldmark_diff')\n",
    "                        ldmark_diff = tf.identity(tf.reshape(scales,[-1,1]) * ldmark_diff, name='ldmark_diff_scaled')\n",
    "\n",
    "\n",
    "\n",
    "                        src_pts = tf.reshape(ldmark_pred, [-1, num_ldmark ,2])\n",
    "                        dst_pts = tf.reshape(ldmark_pred + ldmark_diff, [-1, num_ldmark, 2])\n",
    "\n",
    "                        diff_norm = tf.reduce_mean(tf.norm(src_pts-dst_pts, axis=[1,2]))\n",
    "                        # tf.summary.scalar('diff_norm', diff_norm)\n",
    "                        # tf.summary.scalar('mark', ldmark_pred[0,0])\n",
    "\n",
    "                        images_transformed, dense_flow = sparse_image_warp(warp_input, src_pts, dst_pts,\n",
    "                                regularization_weight = 1e-6, num_boundary_points=0)\n",
    "                        dense_flow = tf.identity(dense_flow, name='dense_flow')\n",
    "\n",
    "                return images_transformed, images_rendered, ldmark_pred, ldmark_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "k6eQRsuz3JbZ",
    "outputId": "9a8052df-0d4d-451b-fc66-a6769a931e4d"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ad7b96e05244>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    " decoder(tf.constant(np.ones((10,256, 256, 3)), dtype=tf.float64), None, tf.constant(np.ones((10, 8))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ALHrXp903Jba"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZMZMTVe3Jba"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "colab": {
   "collapsed_sections": [],
   "name": "decoder.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
