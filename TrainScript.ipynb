{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e9e9c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "169709fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe072059b50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "#%matplotlib inline\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import sys\n",
    "from utils.commandline import load_config\n",
    "from importlib.machinery import SourceFileLoader\n",
    "from datasets.web_caricature import WebCaricatureDataset\n",
    "from losses.model_losses  import AdversarialLoss, PatchAdversarialLoss\n",
    "from models.model_warpgan import WarpGANGenerator, WarpGANDiscriminator\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae056ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4e90fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load configuration file from specified configuration file path\n",
    "config = SourceFileLoader('config', './config/config_train.py').load_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39626731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6128 images of 126 classes loaded.\n",
      "Classes 126: 3016 photos, 3112 caricatures\n",
      "\n",
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can use an image folder dataset the way we have it setup.\n",
    "# Create the dataset\n",
    "dataset = WebCaricatureDataset(config)\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.in_batch, shuffle=True)\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "print(f\"\\nDevice: {device}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8b38749",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "\n",
    "# Create the generator\n",
    "warpgan_generator     = WarpGANGenerator(config).to(device)\n",
    "warpgan_discriminator = WarpGANDiscriminator(config).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6b120b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load losses\n",
    "\n",
    "adversarial_loss       = AdversarialLoss(config)\n",
    "patch_adversarial_loss = PatchAdversarialLoss() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7742d0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Adam optimizers for both G and D\n",
    "\n",
    "optimizerG = optim.Adam(warpgan_generator.parameters(), lr=config.lr, weight_decay=config.weight_decay,\n",
    "                        betas=(config.optimizer[1][\"beta1\"], config.optimizer[1][\"beta2\"]))\n",
    "\n",
    "optimizerD = optim.Adam(warpgan_discriminator.parameters(), lr=config.lr, weight_decay=config.weight_decay,\n",
    "                        betas=(config.optimizer[1][\"beta1\"], config.optimizer[1][\"beta2\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57fb935d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/20][0/3064]\tLoss_G: 20.178382873535156\tLoss_D: 27.349363327026367\t\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/10/_0sx8lw57cs0fmpjcc1jqzr80000gn/T/ipykernel_30330/2415612365.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# calculate gradients for generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mloss_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# optimize generator for single step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/warpgan/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/warpgan/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Training Loop\n",
    "\n",
    "writer = SummaryWriter(\"runs/Train-1\")\n",
    "\n",
    "# For each epoch\n",
    "for epoch in range(config.num_epochs):\n",
    "    \n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "                \n",
    "        global_iter = epoch * len(dataloader) + i\n",
    "            \n",
    "        # ------------------------------------------\n",
    "        # Input dicts for Generator and Disciminator\n",
    "        # ------------------------------------------\n",
    "                        \n",
    "        generator_input_dict = {\n",
    "            \n",
    "            \"images_photo\" : data[\"images_photo\"],\n",
    "            \"images_caric\" : data[\"images_caric\"],\n",
    "            \n",
    "            \"labels_photo\" : data[\"labels_photo\"],\n",
    "            \"labels_caric\" : data[\"labels_caric\"],\n",
    "            \n",
    "            \"scales_photo\" : data[\"scales_photo\"],\n",
    "            \"scales_caric\" : data[\"scales_caric\"],\n",
    "            \n",
    "        }\n",
    "        \n",
    "        discriminator_input_dict = {\n",
    "            \n",
    "            \"images_photo\" : data[\"images_photo\"],\n",
    "            \"images_caric\" : data[\"images_caric\"],\n",
    "            \n",
    "            \"generated_caric\": None\n",
    "            \n",
    "        }\n",
    "        \n",
    "        # ------------------------------------------\n",
    "        # Generator Network Forward Pass\n",
    "        # ------------------------------------------\n",
    "                \n",
    "        # forward pass on generator\n",
    "        generator_output = warpgan_generator(generator_input_dict)\n",
    "        \n",
    "        # add generated caricature to discriminator input dict\n",
    "        discriminator_input_dict[\"generated_caric\"] = generator_output[\"generated_caric\"]\n",
    "                \n",
    "        # forward pass on discriminator\n",
    "        discriminator_output = warpgan_discriminator(discriminator_input_dict)\n",
    "                \n",
    "        # adversial losses on generated caricature\n",
    "        \n",
    "        adversial_loss_input_dict = {\n",
    "\n",
    "                \"logits_caric\" : discriminator_output[\"logits_caric\"],\n",
    "                \"logits_photo\" : discriminator_output[\"logits_photo\"],\n",
    "                \"logits_generated_caric\": discriminator_output[\"logits_generated_caric\"],\n",
    "\n",
    "                \"labels_caric\" : generator_input_dict[\"labels_caric\"],\n",
    "                \"labels_photo\" : generator_input_dict[\"labels_photo\"],\n",
    "                \"labels_generated_caric\" : generator_input_dict[\"labels_photo\"],\n",
    "\n",
    "                }\n",
    "                \n",
    "        loss_DA, loss_GA = adversarial_loss(adversial_loss_input_dict)\n",
    "        \n",
    "        loss_DA, loss_GA = config.coef_adv * loss_DA, config.coef_adv * loss_GA\n",
    "        \n",
    "        # patch adversial losses on generated caricature\n",
    "                \n",
    "        loss_DP, loss_GP = patch_adversarial_loss(discriminator_output[\"logits_caric\"],\n",
    "                                                  discriminator_output[\"logits_photo\"],\n",
    "                                                  discriminator_output[\"logits_generated_caric\"])\n",
    "        \n",
    "        loss_DP, loss_GP = config.coef_adv * loss_DP, config.coef_adv * loss_GP\n",
    "        \n",
    "        # identity mapping (reconstruction) loss\n",
    "                \n",
    "        loss_idt_caric = torch.mean(torch.abs(generator_output[\"rendered_caric\"] - generator_input_dict[\"images_caric\"]))\n",
    "        loss_idt_caric = config.coef_idt * loss_idt_caric\n",
    "        \n",
    "        loss_idt_photo = torch.mean(torch.abs(generator_output[\"rendered_photo\"] - generator_input_dict[\"images_photo\"]))\n",
    "        loss_idt_photo = config.coef_idt * loss_idt_photo\n",
    "        \n",
    "        loss_G_idt     = loss_idt_caric + loss_idt_photo\n",
    "        \n",
    "        # tensorboard writer save all losses\n",
    "                    \n",
    "        writer.add_scalar('Loss-Generator/Adversial',     loss_GA,        global_iter)\n",
    "        writer.add_scalar('Loss-Generator/Patch',         loss_GP,        global_iter)\n",
    "        writer.add_scalar('Loss-Generator/IdentityCaric', loss_idt_caric, global_iter)\n",
    "        writer.add_scalar('Loss-Generator/IdentityPhoto', loss_idt_photo, global_iter)\n",
    "        \n",
    "        writer.add_scalar('Loss-Discriminator/Adversial', loss_DA,        global_iter)\n",
    "        writer.add_scalar('Loss-Discriminator/Patch',     loss_DP,        global_iter)\n",
    "\n",
    "        # collect all losses\n",
    "    \n",
    "        # all losses for generator\n",
    "        loss_G = loss_GA + loss_GP + loss_G_idt\n",
    "        \n",
    "        # all losses for discriminator\n",
    "        loss_D = loss_DA + loss_DP\n",
    "                \n",
    "        # reset gradients of discriminator\n",
    "        warpgan_discriminator.zero_grad()\n",
    "        \n",
    "        # calculate gradients for discriminator\n",
    "        loss_D.backward(retain_graph=True)\n",
    "                \n",
    "        # reset gradients of generator\n",
    "        warpgan_generator.zero_grad()\n",
    "    \n",
    "        # calculate gradients for generator\n",
    "        loss_G.backward()\n",
    "        \n",
    "        # optimize generator for single step\n",
    "        optimizerD.step()\n",
    "        \n",
    "        # optimize generator for single step\n",
    "        optimizerG.step()\n",
    "        \n",
    "        # output training stats\n",
    "        if i % 100 == 0:\n",
    "            \n",
    "            log  = f\"[{epoch}/{config.num_epochs}][{i}/{len(dataloader)}]\\t\"\n",
    "            log += f\"Loss_G: {loss_G}\\t\"\n",
    "            log += f\"Loss_D: {loss_D}\\t\"\n",
    "\n",
    "            print(log)\n",
    "            \n",
    "        # check how the generator is doing by saving G's output\n",
    "        if global_iter % 1 == 0:\n",
    "            \n",
    "            caricature = generator_output[\"generated_caric\"][0].detach().cpu()\n",
    "            writer.add_image(\"Caricature\", caricature, global_iter)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5b9dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1babc491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d8eee5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
